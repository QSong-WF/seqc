import numpy as npimport pandas as pdimport multiprocessingfrom sklearn.neighbors import NearestNeighborsimport timefrom scipy.sparse import csr_matrix, findfrom scipy.sparse.linalg import eigsfrom numpy.linalg import normdef keigs(T, k, P, take_diagonal=0):    """ return k largest magnitude eigenvalues for the matrix T.    :param T: Matrix to find eigen values/vectors of    :param k: number of eigen values/vectors to return    :param P: in the case of symmetric normalizations,              this is the NxN diagonal matrix which relates the nonsymmetric              version to the symmetric form via conjugation    :param take_diagonal: if 1, returns the eigenvalues as a vector rather than as a                          diagonal matrix.    """    D, V = eigs(T, k, tol=1e-4, maxiter=1000)    D = np.real(D)    V = np.real(V)    inds = np.argsort(D)[::-1]    D = D[inds]    V = V[:, inds]    if P is not None:        V = P.dot(V)    # Normalize    for i in range(V.shape[1]):        V[:, i] = V[:, i] / norm(V[:, i])    V = np.round(V, 10)    if take_diagonal == 0:        D = np.diag(D)    return V, Dclass GraphDiffusion:    def __init__(self, knn=10, normalization='smarkov', epsilon=1,                 n_diffusion_components=10):        """        Run diffusion maps on the data. This implementation is based on the        diffusion geometry library in Matlab:        https://services.math.duke.edu/~mauro/code.html#DiffusionGeom and was implemented        by Pooja Kathail        :param data: Data matrix of samples X features        :param knn: Number of neighbors for graph construction to determine distances between cells        :param normalization: method for normalizing the matrix of weights             'bimarkov'            force row and column sums to be 1             'markov'              force row sums to be 1             'smarkov'             symmetric conjugate to markov             'beltrami'            Laplace-Beltrami normalization ala Coifman-Lafon             'sbeltrami'           symmetric conjugate to beltrami             'FokkerPlanck'        Fokker-Planck normalization             'sFokkerPlanck'       symmetric conjugate to Fokker-Planck normalization        :param epsilon: Gaussian standard deviation for converting distances to affinities        :param n_diffusion_components: Number of diffusion components to generate        """        if normalization not in ['bimarkov', 'smarkov', 'markov', 'sbeltrami', 'beltrami',                                 'FokkerPlanck', 'sFokkerPlanck']:            raise ValueError(                'Unsupported normalization. Please refer to the docstring for the '                'supported methods')        self.knn = knn        self.normalization = normalization        self.epsilon = epsilon        self.n_diffusion_components = n_diffusion_components        self.eigenvectors = None        self.eigenvalues = None        self.diffusion_operator = None        self.weights = None    @staticmethod  # todo fix; what is S?    def bimarkov(W, max_iters=100, abs_error=0.00001, verbose=False, **kwargs):        if W.size == 0:            return        # process input        if W.shape[0] != W.shape[1]:            raise ValueError('Bimarkov.py: kernel must be NxN\n')        N = W.shape[0]        # initialize        p = np.ones(N)        # iterative        for i in range(max_iters):            S = np.ravel(S.sum(axis=1)).toarray()            err = np.max(np.absolute(1.0 - np.max(S)), np.absolute(1.0 - np.min(S)))            if err < abs_error:                break            D = csr_matrix((np.divide(1, np.sqrt(S)), (range(N), range(N))), shape=[N, N])            p = S.dot(p)            W = D.dot(W).dot(D)        # iron out numerical errors        T = (W + W.T) / 2        return T, p    @staticmethod    def smarkov(D, N, W):        D = csr_matrix((np.sqrt(D), (range(N), range(N))), shape=[N, N])        P = D        T = D.dot(W).dot(D)        T = (T + T.T) / 2        return T, P    @staticmethod    def markov(D, N, W):        T = csr_matrix((D, (range(N), range(N))), shape=[N, N]).dot(W)        return T, None    @staticmethod    def sbeltrami(D, N, W):        P = csr_matrix((D, (range(N), range(N))), shape=[N, N])        K = P.dot(W).dot(P)        D = np.ravel(K.sum(axis=1))        D[D != 0] = 1 / D[D != 0]        D = csr_matrix((D, (range(N), range(N))), shape=[N, N])        P = D        T = D.dot(K).dot(D)        T = (T + T.T) / 2        return T, P    @staticmethod    def beltrami(D, N, W):        D = csr_matrix((D, (range(N), range(N))), shape=[N, N])        K = D.dot(W).dot(D)        D = np.ravel(K.sum(axis=1))        D[D != 0] = 1 / D[D != 0]        V = csr_matrix((D, (range(N), range(N))), shape=[N, N])        T = V.dot(K)        return T, None    @staticmethod    def FokkerPlanck(D, N, W):        D = csr_matrix((np.sqrt(D), (range(N), range(N))), shape=[N, N])        K = D.dot(W).dot(D)        D = np.ravel(K.sum(axis=1))        D[D != 0] = 1 / D[D != 0]        D = csr_matrix((D, (range(N), range(N))), shape=[N, N])        T = D.dot(K)        return T, None    def sFokkerPlanck(self, D, N, W):        print('(sFokkerPlanck) ... ')        D = csr_matrix((np.sqrt(D), (range(N), range(N))), shape=[N, N])        K = D.dot(W).dot(D)        D = np.ravel(K.sum(axis=1))        D[D != 0] = 1 / D[D != 0]        D = csr_matrix((np.sqrt(D), (range(N), range(N))), shape=[N, N])        P = D        T = D.dot(K).dot(D)        T = (T + T.T) / 2        return T, P    def fit(self, data, verbose=True):        """        :return: Dictionary containing diffusion operator, weight matrix,                 diffusion eigen vectors, and diffusion eigen values        """        if verbose:            print('Running Diffusion maps with the following parameters:')            print('Normalization: %s' % self.normalization)            print('Number of nearest neighbors k: %d' % self.knn)            print('Epsilon: %.4f' % self.epsilon)        # Nearest neighbors        start = time.process_time()        N = data.shape[0]        nbrs = NearestNeighbors(n_neighbors=self.knn).fit(data)        distances, indices = nbrs.kneighbors(data)        # Adjacency matrix        rows = np.zeros(N * self.knn, dtype=np.int32)        cols = np.zeros(N * self.knn, dtype=np.int32)        dists = np.zeros(N * self.knn)        location = 0        for i in range(N):            inds = range(location, location + self.knn)            rows[inds] = indices[i, :]            cols[inds] = i            dists[inds] = distances[i, :]            location += self.knn        W = csr_matrix((dists, (rows, cols)), shape=[N, N])        # Symmetrize W        W = W + W.T        # Convert to affinity (with selfloops)        rows, cols, dists = find(W)        rows = np.append(rows, range(N))        cols = np.append(cols, range(N))        dists = np.append(dists / (self.epsilon ** 2), np.zeros(N))        W = csr_matrix((np.exp(-dists), (rows, cols)), shape=[N, N])        # Create D        D = np.ravel(W.sum(axis=1))        D[D != 0] = 1 / D[D != 0]        # Go through the various normalizations        fnorm = getattr(self, self.normalization)        T, P = fnorm(D=D, N=N, W=W)        if self.normalization != 'bimarkov' and verbose:            print('%.2f seconds' % (time.process_time() - start))        # Eigen value decomposition        V, D = keigs(T, self.n_diffusion_components, P, take_diagonal=1)        self.eigenvectors = V        self.eigenvalues = D        self.diffusion_operator = T        self.weights = Wclass PCA:    def __init__(self, n_components=100):        self.n_components = n_components        self.loadings = None        self.eigenvalues = None    def fit(self, data):        if isinstance(data, pd.DataFrame):            X = data.values        elif isinstance(data, np.ndarray):            X = data        else:            raise TypeError('data must be a pd.DataFrame or np.ndarray')        # Make sure data is zero mean        X = np.subtract(X, np.amin(X))        X = np.divide(X, np.amax(X))        # Compute covariance matrix        if X.shape[1] < X.shape[0]:            C = np.cov(X, rowvar=0)        # if N > D, we better use this matrix for the eigendecomposition        else:            C = np.multiply((1 / X.shape[0]), np.dot(X, X.T))        # Perform eigendecomposition of C        C[np.where(np.isnan(C))] = 0        C[np.where(np.isinf(C))] = 0        l, M = np.linalg.eig(C)        # Sort eigenvectors in descending order        ind = np.argsort(l)[::-1]        l = l[ind]        if self.n_components < 1:            self.n_components = (                np.where(np.cumsum(np.divide(l, np.sum(l)), axis=0) >=                         self.n_components)[0][0] + 1)            print('Embedding into ' + str(self.n_components) + ' dimensions.')        elif self.n_components > M.shape[1]:            self.n_components = M.shape[1]            print('Target dimensionality reduced to ' + str(self.n_components) + '.')        M = M[:, ind[:self.n_components]]        l = l[:self.n_components]        # Apply mapping on the data        if X.shape[1] >= X.shape[0]:            M = np.multiply(np.dot(X.T, M), (1 / np.sqrt(X.shape[0] * l)).T)        self.loadings = M        self.eigenvalues = l    def transform(self, data, n_components=None):        if n_components is None:            n_components = self.n_components        projected = np.dot(data, self.loadings[:, :n_components])        if isinstance(data, pd.DataFrame):            return pd.DataFrame(projected, index=data.index)        else:            return projected    def fit_transform(self, data):        self.fit(data)        return self.transform(data)class correlation:    @staticmethod    def vector(x: np.array, y: np.array):        x = x[:, np.newaxis]        mu_x = x.mean()  # cells        mu_y = y.mean(axis=0)  # cells by gene --> cells by genes        sigma_x = x.std()        sigma_y = y.std(axis=0)        return ((y * x).mean(axis=0) - mu_y * mu_x) / (sigma_y * sigma_x)    @staticmethod    def map(x, y):        """Correlate each n with each m.        :param x: np.array; shape N x T.        :param y: np.array; shape M x T.        :returns: np.array; shape N x M in which each element is a correlation                            coefficient.        """        mu_x = x.mean(1)        mu_y = y.mean(1)        n = x.shape[1]        if n != y.shape[1]:            raise ValueError('x and y must ' +                             'have the same number of timepoints.')        s_x = x.std(1, ddof=n - 1)        s_y = y.std(1, ddof=n - 1)        cov = np.dot(x,                     y.T) - n * np.dot(mu_x[:, np.newaxis],                                       mu_y[np.newaxis, :])        return cov / np.dot(s_x[:, np.newaxis], s_y[np.newaxis, :])    @staticmethod    def eigv(evec, data, components=tuple(), knn=10):        if isinstance(data, pd.DataFrame):            df = True            D = data        elif isinstance(data, np.ndarray):            D = data            df = False        else:            raise TypeError('data must be a pd.DataFrame or np.ndarray')        # set components, remove zero if it was specified        if not components:            components = np.arange(evec.shape[1])        else:            components = np.array(components)        components = components[components != 0]        eigv_corr = np.empty((D.shape[1],                              evec.shape[1]),                             dtype=np.float)        # assert eigv_corr.shape == (gene_shape, component_shape),        #     '{!r}, {!r}'.format(eigv_corr.shape,        #                         (gene_shape, component_shape))        for component_index in components:            component_data = evec[:, component_index]            # assert component_data.shape == (cell_shape,), '{!r} != {!r}'.format(            #     component_data.shape, (cell_shape,))            order = np.argsort(component_data)            x = pd.rolling_mean(component_data[order], knn)[knn:]            # assert x.shape == (cell_shape - no_cells,)            # this fancy indexing will copy self.molecules            vals = pd.rolling_mean(D[order, :], knn, axis=0)[                   knn:]            # assert vals.shape == (cell_shape - no_cells, gene_shape)            eigv_corr[:, component_index] = correlation(x, vals)        # this is sorted by order, need it in original order (reverse the sort)        eigv_corr = eigv_corr[:, components]        if df:            eigv_corr = pd.DataFrame(                eigv_corr[:, components], index=data.columns,                columns=components)        return eigv_corrclass smoothing:    @staticmethod    def kneighbors(data, n_neighbors=50):        """        :param data: np.ndarray | pd.DataFrame; genes x cells array        :param n_neighbors: int; number of neighbors to smooth over        :return: np.ndarray | pd.DataFrame; same as input        """        if isinstance(data, pd.DataFrame):            df = True            data_ = data.values        elif isinstance(data, np.ndarray):            df = False            data_ = data        else:            raise TypeError("data must be a pd.DataFrame or np.ndarray")        knn = NearestNeighbors(            n_neighbors=n_neighbors,            n_jobs=multiprocessing.cpu_count() - 1)        knn.fit(data_)        dist, inds = knn.kneighbors(data_)        # set values equal to their means across neighbors        res = data_[inds, :].mean(axis=1)        if df:            res = pd.DataFrame(res, index=data.index,                               columns=data.columns)        return res