#!/usr/local/bin/python3
__author__ = 'Ambrose J. Carr'

import argparse
import pickle
import os
import shutil
from copy import copy
from seqc.fastq import merge_fastq
from seqc.align import STAR
from seqc.barcodes import DropSeqCellBarcodes
from seqc.arrays import ReadArray
from seqc.convert_features import construct_feature_table
import numpy as np
import sys
import logging
import json
from datetime import datetime


# todo | add ability to fetch genome from amazon (provide s3 link OR file path)
# todo | add ability to fetch data from geo (provide additional alt. input argument -g)
# todo | test index generation (pull from seqdb)
# todo | merge all of the metadata into a dictionary that is written at the end of the
# todo |   pipeline or when any errors occur
# todo | add ability to detect and quantify genomic/mitochondrial/other non-tx alignments
# todo | change p_coalignment.pckl to an array pickle (as long as this doesn't use too
# todo |   much memory
# todo | add sub-commands to allow running drop-seq only, etc. This is done through sub-
# todo |   parsers, whose names can be identified in the namespace as subparser_name
def process_input():
    parser = argparse.ArgumentParser()

    # add subparsers for each library construction method
    subparsers = parser.add_subparsers(help='library construction method types',
                                       dest='subparser_name')
    parse_in_drop = subparsers.add_parser('in-drop', help='in-drop help')
    parse_drop_seq = subparsers.add_parser('drop-seq', help='drop-seq help')
    parse_mars_seq = subparsers.add_parser('mars-seq', help='mars-seq help')
    parse_cel_seq = subparsers.add_parser('cel-seq', help='cel-seq help')
    parse_avo_seq = subparsers.add_parser('avo-seq', help='avo-seq help')
    parse_strt_seq = subparsers.add_parser('strt-seq', help='strt-seq help')

    # get a list of parsers, set-up pipeline function for each parser
    subparser_list = [parse_in_drop, parse_mars_seq, parse_cel_seq,
                      parse_avo_seq, parse_strt_seq, parse_drop_seq]
    default_functions = [in_drop, mars_seq, cel_seq, avo_seq, strt_seq, drop_seq]
    for p, f in zip(subparser_list, default_functions):
        p.set_defaults(func=f)

    # set barcode default for drop_seq:
    parse_drop_seq.set_defaults(barcodes=[])

    # set required arguments for all parsers
    for i, p in enumerate(subparser_list):
        r = p.add_argument_group('Required Arguments')
        # todo needs to take input from user on what organism it should be
        r.add_argument('-i', '--index', metavar='I', help='star alignment index folder. '
                       'This folder will be created if it does not exist', default=None)
        r.add_argument('-n', '--n-threads', help='number of threads to run', metavar='N',
                       type=int, default=None)
        r.add_argument('-o', '--output-file', metavar='O', default=None,
                       help='stem of filename in which to store output')

        # for all experiments except drop-seq, barcodes are a required input argument
        if i < 5:
            r.add_argument('-b', '--barcodes', metavar='B', default=None,
                           help='location of serialized barcode object.')

        i = p.add_argument_group(
            title='Input Files',
            description='pass one input file type: sam (-s), raw fastq (-f, [-r]), or '
                        'processed fastq (-m)')

        i.add_argument('-f', '--forward', help='forward fastq file(s)', metavar='F',
                       nargs='*', default=None)
        i.add_argument('-r', '--reverse', help='reverse fastq file(s)', metavar='R',
                       nargs='*', default=None)
        i.add_argument('-s', '--sam', metavar='S', nargs='?', default=None,
                       help='sam file(s) containing aligned, pre-processed reads')
        i.add_argument('-m', '--merged-fastq', metavar='M', default=None,
                       help='fastq file containing merged, pre-processed records')

        # disambiguation arguments
        d = p.add_argument_group('Optional arguments for disambiguation')
        d.add_argument('-l', '--frag-len', metavar='L', type=int, default=1000,
                       help='the number of bases from the 3 prime end to '
                       'consider when determining trancript overlaps')

        # alignment arguments
        a = p.add_argument_group('Optional arguments for STAR aligner')
        a.add_argument('--star-args', metavar='SA', nargs='+', default={},
                       help='additional arguments for STAR. Pass as arg=value without '
                            'leading "--". e.g. runMode=alignReads')
        a.add_argument('--list-default-star-args', default=False, action='store_true',
                       help='list SEQDB default args for the STAR aligner')

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(2)

    arguments = parser.parse_args()

    # list star args if requested, then exit
    if arguments.list_default_star_args:
        printable_args = json.dumps(
            STAR.default_alignment_args('$FASTQ', '$N_THREADS', '$INDEX', './$EXP_NAME/'),
            separators=(',', ': '), indent=4)
        print(printable_args)
        sys.exit(2)

    # check that at least one input argument was passed:
    check = [arguments.forward, arguments.reverse, arguments.sam, arguments.merged_fastq]
    if not any(check):
        print('SEQC %s: error: one or more of the following arguments must be provided: '
              '-f/--forward, -r/--reverse, -m/--merged-fastq, -s/--sam' %
              arguments.subparser_name)
        sys.exit(2)
    required = [arguments.output_file, arguments.index, arguments.n_threads]
    if not arguments.subparser_name == 'drop-seq':
        if not all(required + [arguments.barcodes]):
            print('SEQC %s: error: the following arguments are required: -i/--index, -n/'
                  '--n-threads, -o/--output-file, -b/--barcodes')
            sys.exit(2)
    else:
        if not all(required):
            print('SEQC %s: error: the following arguments are required: -i/--index, -n/'
                  '--n-threads, -o/--output-file')

    return arguments


def setup_logger():
    """create a simple log file in the cwd to track progress and any errors"""
    logging.basicConfig(filename='seqc.log', level=logging.DEBUG)


def log_info(message):
    """print a timestamped update for the user"""
    logging.info(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':' + message)


def set_up(fout, index, barcodes):
    """
    create temporary directory for run, find gtf in index, and create or load a
    serialized barcode object
    """

    # create temporary directory based on experiment name
    temp_dir = '.' + fout
    if not os.path.isdir(temp_dir):
        os.makedirs(temp_dir)
    if not temp_dir.endswith('/'):
        temp_dir += '/'

    # obtain gtf file from index argument
    gtf = index + 'annotations.gtf'
    if not os.path.isfile(gtf):
        raise FileNotFoundError('no file named "annotations.gtf" found in index')

    # get cell barcode files
    if not barcodes:
        cb = DropSeqCellBarcodes()
    else:
        with open(barcodes, 'rb') as f:
            cb = pickle.load(f)

    return temp_dir, gtf, cb


def merge(fout, forward, reverse, samfile, merged_fastq, processor, temp_dir, cb):
    if (forward or reverse) and not (samfile or merged_fastq):

        # pre-process reads
        log_info('Merging fastq files.')
        merged_fastq, nlc = merge_fastq(forward, reverse, processor, temp_dir, cb)

    with open(fout + '_metadata.txt', 'a') as f:
        f.write('Low Complexity Reads: %d\n' % nlc)

    return merged_fastq


def align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, fout):
    if merged_fastq and not samfile:
        # process any additional arguments for star passed from the command line
        if star_args:
            kwargs = {}
            for arg in star_args:
                k, v = arg.split('=')
                kwargs['--' + k] = v

        # align fastq files
        log_info('Aligning merged fastq file')
        STAR.align(merged_fastq, index, n_threads, temp_dir, **star_args)
        samfile = temp_dir + 'Aligned.out.sam'

        # copy alignment summary
        shutil.copyfile(temp_dir + 'Log.final.out', fout + '_alignment_summary.txt')
        return samfile


def process_samfile(samfile, gtf, frag_len):
    log_info('Post-processing alignments')
    ft, fp = construct_feature_table(gtf, frag_len)
    arr = ReadArray.from_samfile(samfile, ft, fp)
    return arr


def correct_errors():
    # try:
    #     log_info('Correcting errors')
    #     if not barcode_dir.endswith('/'):
    #         barcode_dir += '/'
    #     barcode_files = glob(barcode_dir + '*')
    #     corrected, nerr, err_rate = correct_errors(
    #         unambiguous, barcode_files, processor_name, meta, p_val=0.1)
    #
    #     # store metadata from these runs in post-processing summary
    #     # post_processing_summary(h5db, exp_name, dis_meta, nerr)
    #
    #     # store error rates
    #     # store_error_rate(h5db, exp_name, err_rate)
    #
    # except:  # fail gracefully on error_correction; not all methods can do this!
    #     logging.exception(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':main:')
    #     corrected = unambiguous
    raise NotImplementedError


def resolve_alignments(index, arr, n, fout):
    log_info('Resolving ambiguous alignments.')
    try:
        # todo
        # I should store this in a jagged array as well. Indexing is 1/2 as fast
        # but it would still be very quick, and VERY memory efficient.
        expectations = index + 'p_coalignment_array.p'
        arr.resolve_alignments(expectations, required_poly_t=n)
    except:
        log_info('Caught error in resolve_alignments(), saving data')
        arr.save_h5(fout + '.h5')
        raise


def save_counts_matrices(fout, arr, n):
    log_info('Generating Gene x Cell SparseMatrices for reads and molecules.')
    mols, mrow, mcol = arr.unique_features_to_sparse_counts(
        collapse_molecules=True, n_poly_t_required=n)
    reads, rrow, rcol = arr.unique_features_to_sparse_counts(
        collapse_molecules=False, n_poly_t_required=n)
    matrices = {'molecules': {'matrix': mols, 'row_ids': mrow, 'col_ids': mcol},
                'reads': {'matrix': reads, 'row_ids': rrow, 'col_ids': rcol}}
    with open(fout + '_read_and_mol_matrices.p', 'wb') as f:
        pickle.dump(matrices, f)


def select_cells():
    ################################### SELECT CELLS ####################################
    # # filter cells with < threshold # reads. todo | add gc, length, maybe size biases
    # mols, meta = filter_cells_mols(mols, meta, 10)
    # reads, meta = filter_cells_reads(reads, meta, 100)
    #
    # # store files
    # if location:
    #     location += '/'
    # with open(location + exp_name + '_read_and_mol_frames.p', 'wb') as f:
    #     pickle.dump(((reads, rrow, rcol), (mols, mrow, mcol)), f)
    raise NotImplementedError


def store_results(fout, arr):
    if not fout.endswith('.h5'):
        fout += '.h5'
    log_info('Storing processed data in %s' % fout)
    arr.save_h5(fout)


def run_complete():
    log_info('Run complete.')


def clean_up(temp_dir):
    log_info('Run succeeded. Cleaning up and terminating.')
    if os.path.isdir(temp_dir):
        shutil.rmtree(temp_dir)
    if os.path.isdir('_STARtmp'):
        shutil.rmtree('_STARtmp')


def in_drop(fout, forward, reverse, samfile, merged_fastq, processor, index,
            n_threads, frag_len, star_args, barcodes):

    temp_dir, gtf, cb = set_up(fout, index, barcodes)

    merged_fastq = merge(fout, forward, reverse, samfile, merged_fastq, processor,
                         temp_dir, cb)

    samfile = align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, fout)

    arr = process_samfile(samfile, gtf, frag_len)

    resolve_alignments(index, arr, n=0, fout=fout)

    store_results(fout, arr)

    save_counts_matrices(fout, arr, n=3)

    # clean_up(temp_dir)

    run_complete()


def drop_seq(fout, forward, reverse, samfile, merged_fastq, processor, index,
             n_threads, frag_len, star_args, barcodes):

    temp_dir, gtf, cb = set_up(fout, index, barcodes)

    merged_fastq = merge(fout, forward, reverse, samfile, merged_fastq, processor,
                         temp_dir, cb)

    samfile = align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, fout)

    arr = process_samfile(samfile, gtf, frag_len)

    resolve_alignments(index, arr, n=0, fout=fout)

    arr.save_h5(fout + '.h5')

    save_counts_matrices(fout, arr, n=0)

    # clean_up(temp_dir)

    run_complete()


def mars_seq():
    raise NotImplementedError


def cel_seq():
    raise NotImplementedError


def avo_seq():
    raise NotImplementedError


def strt_seq():
    raise NotImplementedError


if __name__ == "__main__":
    args = process_input()
    setup_logger()
    try:
        # log command line arguments for debugging
        arg_dict = copy(vars(args))
        del arg_dict['func']  # function is not serializable
        log_info('Passed command line arguments: %s' %
                 json.dumps(arg_dict, separators=(',', ': '), indent=4))
        args.func(args.output_file, args.forward, args.reverse, args.sam,
                  args.merged_fastq, args.subparser_name, args.index, args.n_threads,
                  args.frag_len, args.star_args, args.barcodes)
    except:
        logging.exception(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':main:')
        raise
