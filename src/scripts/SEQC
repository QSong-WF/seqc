#!/usr/local/bin/python3
__author__ = 'Ambrose J. Carr'

import argparse
import pickle
import os
import shutil
from copy import copy
from seqc.io_lib import S3
from seqc.fastq import merge_fastq
from seqc.align import STAR, _download_links
from seqc.barcodes import DropSeqCellBarcodes
from seqc.arrays import ReadArray
from seqc.log import setup_logger, log_info, log_exception
from seqc.convert_features import ConvertFeatureCoordinates
import numpy as np
import sys
import json
# import logging
# from datetime import datetime


# todo | add ability to fetch data from geo (provide additional alt. input argument -g)
# todo | merge all of the metadata into a dictionary that is written at the end of the
# todo |   pipeline or when any errors occur
# todo | add ability to detect and quantify genomic/mitochondrial/other non-tx alignments
# todo | SLOWEST PIECES ARE:
# todo | (1) post-process -- this can be sped up by generating in pieces, out of memory
# todo |     pandas table using all 30 threads. This will drop the time ~ 30x; somewhat
# todo |     tricky to generate the JaggedArrays; think on this!
# todo | (2) disambiguation -- profile to check for slow part. One thing that will speed
# todo |     up the process is sparsifying the arrays; eliminating zeros. Will reduce
# todo |     overhead by a lot.
def process_input():
    parser = argparse.ArgumentParser()

    # add subparsers for each library construction method
    subparsers = parser.add_subparsers(help='library construction method types',
                                       dest='subparser_name')
    parse_in_drop = subparsers.add_parser('in-drop', help='in-drop help')
    parse_drop_seq = subparsers.add_parser('drop-seq', help='drop-seq help')
    parse_mars_seq = subparsers.add_parser('mars-seq', help='mars-seq help')
    parse_cel_seq = subparsers.add_parser('cel-seq', help='cel-seq help')
    parse_avo_seq = subparsers.add_parser('avo-seq', help='avo-seq help')
    parse_strt_seq = subparsers.add_parser('strt-seq', help='strt-seq help')

    # get a list of parsers, set-up pipeline function for each parser
    subparser_list = [parse_in_drop, parse_mars_seq, parse_cel_seq,
                      parse_avo_seq, parse_strt_seq, parse_drop_seq]
    default_functions = [in_drop, mars_seq, cel_seq, avo_seq, strt_seq, drop_seq]
    for p, f in zip(subparser_list, default_functions):
        p.set_defaults(func=f)

    # set barcode default for drop_seq:
    parse_drop_seq.set_defaults(barcodes=[])

    # set required arguments for all parsers
    for i, p in enumerate(subparser_list):
        r = p.add_argument_group('Required Arguments')
        # todo needs to take input from user on what organism it should be
        r.add_argument('-i', '--index', metavar='I', help='local or s3 location of star '
                       'alignment index folder. This folder will be created if it does '
                       'not exist', default=None)
        r.add_argument('-n', '--n-threads', help='number of threads to run', metavar='N',
                       type=int, default=None)
        r.add_argument('-o', '--output-file', metavar='O', default=None,
                       help='stem of filename in which to store output')

        # for all experiments except drop-seq, barcodes are a required input argument
        if i < 5:
            r.add_argument('-b', '--barcodes', metavar='B', default=None,
                           help='local or s3 location of serialized barcode object.')

        i = p.add_argument_group(
            title='Input Files',
            description='pass one input file type: sam (-s), raw fastq (-f, [-r]), or '
                        'processed fastq (-m)')

        i.add_argument('-f', '--forward', help='forward fastq file(s)', metavar='F',
                       nargs='*', default=None)
        i.add_argument('-r', '--reverse', help='reverse fastq file(s)', metavar='R',
                       nargs='*', default=None)
        i.add_argument('-s', '--samfile', metavar='S', nargs='?', default=None,
                       help='sam file(s) containing aligned, pre-processed reads')
        i.add_argument('-m', '--merged-fastq', metavar='M', default=None,
                       help='fastq file containing merged, pre-processed records')

        # disambiguation arguments
        d = p.add_argument_group('Optional arguments for disambiguation')
        d.add_argument('-l', '--frag-len', metavar='L', type=int, default=1000,
                       help='the number of bases from the 3 prime end to '
                       'consider when determining trancript overlaps')

        # alignment arguments
        a = p.add_argument_group('Optional arguments for STAR aligner')
        a.add_argument('--star-args', metavar='SA', nargs='+', default={},
                       help='additional arguments for STAR. Pass as arg=value without '
                            'leading "--". e.g. runMode=alignReads')
        a.add_argument('--list-default-star-args', default=False, action='store_true',
                       help='list SEQDB default args for the STAR aligner')

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(2)

    # add a sub-parser for building the index
    pindex = subparsers.add_parser('index', help='SEQC index functions')
    pindex.add_argument('-b', '--build', action='store_true', default=False,
                        help='build a SEQC index')
    pindex.add_argument('-t', '--test', action='store_true', default=False,
                        help='test a SEQC index')
    pindex.add_argument('-o', '--organism', required=True, nargs='+', metavar='O',
                        help='build index for these organism(s)')
    pindex.add_argument('-i', '--index', help='name of folder where index should be '
                        'built or containing the index to be verified',
                        required=True, metavar='I')
    pindex.add_argument('-n', '--n-threads', type=int, default=4, help='number of threads'
                        ' to use when building index', metavar='N')
    # todo | phiX is going to need an SCID! will this be automatically generated? Is it
    # todo |  going to get fucked up by all the post-processing last 1kb stuff?
    # todo | change p_coalignment to array
    pindex.add_argument('--phix', help='add phiX to the genome index and GTF file.',
                        action='store_true', default=False)

    arguments = parser.parse_args()

    if arguments.subparser_name == 'index':
        if arguments.build and not arguments.test:
            arguments.func = STAR.build_index
        elif arguments.test and not arguments.build:
            arguments.func = STAR.test_index
        else:
            print('SEQC index: error: one but not both of the following arguments must '
                  'be provided: -b/--build, -t/--test')
            sys.exit(2)
    else:

        # list star args if requested, then exit
        if arguments.list_default_star_args:
            printable_args = json.dumps(
                STAR.default_alignment_args('$FASTQ', '$N_THREADS', '$INDEX', './$EXP_NAME/'),
                separators=(',', ': '), indent=4)
            print(printable_args)
            sys.exit(2)

        # check that at least one input argument was passed:
        check = [arguments.forward, arguments.reverse, arguments.samfile,
                 arguments.merged_fastq]
        if not any(check):
            print('SEQC %s: error: one or more of the following arguments must be '
                  'provided: -f/--forward, -r/--reverse, -m/--merged-fastq, -s/--sam' %
                  arguments.subparser_name)
            sys.exit(2)
        required = [arguments.output_file, arguments.index, arguments.n_threads]
        if not arguments.subparser_name == 'drop-seq':
            if not all(required + [arguments.barcodes]):
                print('SEQC %s: error: the following arguments are required: -i/--index, '
                      '-n/--n-threads, -o/--output-file, -b/--barcodes')
                sys.exit(2)
        else:
            if not all(required):
                print('SEQC %s: error: the following arguments are required: -i/--index, '
                      '-n/--n-threads, -o/--output-file')

    return vars(arguments)


# def setup_logger():
#     """create a simple log file in the cwd to track progress and any errors"""
#     logging.basicConfig(filename='seqc.log', level=logging.DEBUG)
#
#
# def log_info(message):
#     """print a timestamped update for the user"""
#     logging.info(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':' + message)


def set_up(output_file, index, barcodes):
    """
    create temporary directory for run, find gtf in index, and create or load a
    serialized barcode object
    """

    # temporary directory should be made in the same directory as the output prefix
    *stem, final_dir = output_file.split('/')
    temp_dir = '/'.join(stem + ['.' + final_dir])

    # create temporary directory based on experiment name
    if not os.path.isdir(temp_dir):
        os.makedirs(temp_dir)
    if not temp_dir.endswith('/'):
        temp_dir += '/'

    # check that index exists. If index is an aws link, download the index
    if index.startswith('s3://'):
        log_info('AWS s3 link provided for index. Downloading index.')
        bucket, prefix = S3.split_link(index)
        index = temp_dir + 'index/'
        cut_dirs = prefix.count('/')
        S3.download_files(bucket, prefix, index, cut_dirs)

    # obtain gtf file from index argument
    gtf = index + 'annotations.gtf'
    if not os.path.isfile(gtf):
        raise FileNotFoundError('no file named "annotations.gtf" found in index')

    # get cell barcode files
    if not barcodes:
        cb = DropSeqCellBarcodes()
    else:
        if barcodes.startswith('s3://'):
            log_info('AWS s3 link provided for barcodes. Downloading barcodes')
            bucket, key = S3.split_link(barcodes)
            output_file = temp_dir + 'barcodes.p'
            try:
                barcodes = S3.download_file(bucket, key, output_file)
            except FileExistsError:
                barcodes = output_file
                pass  # already have the file in this location from a previous run

        # now, we should definitely have the binary file. Load it.
        with open(barcodes, 'rb') as f:
            cb = pickle.load(f)

    return temp_dir, gtf, cb, index


def merge(output_file, forward, reverse, samfile, merged_fastq, processor, temp_dir, cb):
    if (forward or reverse) and not (samfile or merged_fastq):

        # pre-process reads
        log_info('Merging fastq files.')
        merged_fastq, nlc = merge_fastq(forward, reverse, processor, temp_dir, cb)

    with open(output_file + '_metadata.txt', 'a') as f:
        f.write('Low Complexity Reads: %d\n' % nlc)

    return merged_fastq


def align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, output_file):
    if merged_fastq and not samfile:
        # process any additional arguments for star passed from the command line
        if star_args:
            kwargs = {}
            for arg in star_args:
                k, v = arg.split('=')
                kwargs['--' + k] = v

        # align fastq files
        log_info('Aligning merged fastq file')
        STAR.align(merged_fastq, index, n_threads, temp_dir, **star_args)
        samfile = temp_dir + 'Aligned.out.sam'

        # copy alignment summary
        shutil.copyfile(temp_dir + 'Log.final.out', output_file + '_alignment_summary.txt')
        return samfile


def process_samfile(samfile, gtf, frag_len):
    log_info('Post-processing alignments')
    fc = ConvertFeatureCoordinates.from_gtf(gtf, frag_len)
    arr = ReadArray.from_samfile(samfile, fc)
    return arr


def correct_errors():
    # try:
    #     log_info('Correcting errors')
    #     if not barcode_dir.endswith('/'):
    #         barcode_dir += '/'
    #     barcode_files = glob(barcode_dir + '*')
    #     corrected, nerr, err_rate = correct_errors(
    #         unambiguous, barcode_files, processor_name, meta, p_val=0.1)
    #
    #     # store metadata from these runs in post-processing summary
    #     # post_processing_summary(h5db, exp_name, dis_meta, nerr)
    #
    #     # store error rates
    #     # store_error_rate(h5db, exp_name, err_rate)
    #
    # except:  # fail gracefully on error_correction; not all methods can do this!
    #     logging.exception(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':main:')
    #     corrected = unambiguous
    raise NotImplementedError


def resolve_alignments(index, arr, n, output_file):
    log_info('Resolving ambiguous alignments.')
    try:
        # todo
        # I should store this in a jagged array as well. Indexing is 1/2 as fast
        # but it would still be very quick, and VERY memory efficient.
        expectations = index + 'p_coalignment_array.p'
        arr.resolve_alignments(expectations, required_poly_t=n)
    except:
        log_info('Caught error in resolve_alignments(), saving data')
        arr.save_h5(output_file + '.h5')
        raise


def save_counts_matrices(output_file, arr, n):

#### DEBUGGING TRY/EXCEPT; remove when clear what the problem with this error is ####
# Traceback (most recent call last):
#   File "/data/seqc/src/scripts/SEQC", line 419, in <module>
#     func(**kwargs)
#   File "/data/seqc/src/scripts/SEQC", line 359, in in_drop
#     save_counts_matrices(output_file, arr, n=3)
#   File "/data/seqc/src/scripts/SEQC", line 301, in save_counts_matrices
#     collapse_molecules=True, n_poly_t_required=n)
#   File "/data/seqc/src/seqc/arrays.py", line 1030, in unique_features_to_sparse_counts
#     low_coverage_mask = self.mask_low_support_molecules()
#   File "/data/seqc/src/seqc/arrays.py", line 850, in mask_low_support_molecules
#     df = pd.DataFrame(view)
#   File "/usr/local/lib/python3.5/site-packages/pandas/core/frame.py", line 238, in __init__
#     if mask.any():
#   File "/usr/local/lib/python3.5/site-packages/numpy/core/_methods.py", line 38, in _any
#     return umr_any(a, axis, dtype, out, keepdims)
# TypeError: cannot perform reduce with flexible type

    try:
        log_info('Generating Gene x Cell SparseMatrices for reads and molecules.')
        mols, mrow, mcol = arr.unique_features_to_sparse_counts(
            collapse_molecules=True, n_poly_t_required=n)
        reads, rrow, rcol = arr.unique_features_to_sparse_counts(
            collapse_molecules=False, n_poly_t_required=n)
        matrices = {'molecules': {'matrix': mols, 'row_ids': mrow, 'col_ids': mcol},
                    'reads': {'matrix': reads, 'row_ids': rrow, 'col_ids': rcol}}
        with open(output_file + '_read_and_mol_matrices.p', 'wb') as f:
            pickle.dump(matrices, f)
    except TypeError:
        all_objects = dir()
        log_info('all array objects:' + repr(all_objects))
        for obj in all_objects:
            if isinstance(obj, np.ndarray):
                log_info('Raised TypeError; logging all array object types and shapes')
                log_info(repr(obj))
                log_info(repr(obj.dtype))
                log_info(repr(obj.shape))
        raise


def select_cells():
    ################################### SELECT CELLS ####################################
    # # filter cells with < threshold # reads. todo | add gc, length, maybe size biases
    # mols, meta = filter_cells_mols(mols, meta, 10)
    # reads, meta = filter_cells_reads(reads, meta, 100)
    #
    # # store files
    # if location:
    #     location += '/'
    # with open(location + exp_name + '_read_and_mol_frames.p', 'wb') as f:
    #     pickle.dump(((reads, rrow, rcol), (mols, mrow, mcol)), f)
    raise NotImplementedError


def store_results(output_file, arr):
    if not output_file.endswith('.h5'):
        output_file += '.h5'
    log_info('Storing processed data in %s' % output_file)
    arr.save_h5(output_file)


def run_complete():
    log_info('Run complete.')


def clean_up(temp_dir):
    log_info('Run succeeded. Cleaning up and terminating.')
    if os.path.isdir(temp_dir):
        shutil.rmtree(temp_dir)
    if os.path.isdir('_STARtmp'):
        shutil.rmtree('_STARtmp')


def in_drop(output_file, forward, reverse, samfile, merged_fastq, subparser_name, index,
            n_threads, frag_len, star_args, barcodes, **kwargs):

    temp_dir, gtf, cb, index = set_up(output_file, index, barcodes)

    merged_fastq = merge(output_file, forward, reverse, samfile, merged_fastq, subparser_name,
                         temp_dir, cb)

    samfile = align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, output_file)

    arr = process_samfile(samfile, gtf, frag_len)

    resolve_alignments(index, arr, n=0, output_file=output_file)

    store_results(output_file, arr)

    save_counts_matrices(output_file, arr, n=3)

    # clean_up(temp_dir)

    run_complete()


def drop_seq(output_file, forward, reverse, samfile, merged_fastq, subparser_name, index,
             n_threads, frag_len, star_args, barcodes, **kwargs):

    temp_dir, gtf, cb, index = set_up(output_file, index, barcodes)

    merged_fastq = merge(output_file, forward, reverse, samfile, merged_fastq, subparser_name,
                         temp_dir, cb)

    samfile = align(merged_fastq, samfile, star_args, temp_dir, n_threads, index, output_file)

    arr = process_samfile(samfile, gtf, frag_len)

    resolve_alignments(index, arr, n=0, output_file=output_file)

    arr.save_h5(output_file + '.h5')

    save_counts_matrices(output_file, arr, n=0)

    # clean_up(temp_dir)

    run_complete()


def mars_seq():
    raise NotImplementedError


def cel_seq():
    raise NotImplementedError


def avo_seq():
    raise NotImplementedError


def strt_seq():
    """
    This method assumes STRT-seq datasets are being retrieved from GEO, where Sten
    Linnarsson has demultiplexed the data into multiple, individual fastq files."""
    raise NotImplementedError


if __name__ == "__main__":
    kwargs = process_input()
    setup_logger()
    try:
        # log command line arguments for debugging
        arg_copy = copy(kwargs)
        del arg_copy['func']  # function is not serializable
        log_info('SEQC working directory: %s' % os.getcwd())
        log_info('Passed command line arguments: %s' %
                 json.dumps(arg_copy, separators=(',', ': '), indent=4))

        func = kwargs['func']
        func(**kwargs)

        # args.func(args.output_file, args.forward, args.reverse, args.sam,
        #           args.merged_fastq, args.subparser_name, args.index, args.n_threads,
        #           args.frag_len, args.star_args, args.barcodes)
    except:
        log_exception()
        # logging.exception(datetime.now().strftime("%Y-%m-%d %H:%M:%S") + ':main:')
        raise
